version : '3.1'

services:

  cassandra:
    container_name: cassandra
    image: "cassandra:3.0.12"
    ports:
      - "9042:9042"

  postgres:
    image: "postgres:9.6.5"
    container_name: "postgres"
    ports:
      - "5432:5432"
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: testDB
      POSTGRES_PASSWORD: test
      POSTGRES_USER: test

  spark-master:
    image: bde2020/spark-master:2.2.0-hadoop2.7
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ../build:/usr/build
      - spark:/spark
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - "constraint:node==spark-master"

  spark-worker:
    image: bde2020/spark-worker:2.2.0-hadoop2.7
    container_name: spark-worker1
#    deploy:
#      replicas: 2
    links:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "constraint:node==spark-master"

  spark-worker1:
    image: bde2020/spark-worker:2.2.0-hadoop2.7
    container_name: spark-worker2
#    deploy:
#      replicas: 2
    links:
      - spark-master
    ports:
      - "8082:8082"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "constraint:node==spark-master"

  spark-worker2:
    image: bde2020/spark-worker:2.2.0-hadoop2.7
    container_name: spark-worker3
#    deploy:
#      replicas: 2
    links:
      - spark-master
    ports:
      - "8083:8083"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "constraint:node==spark-master"

  spark-batchjob-1:
    image: java:8-jre
#    restart: on-failure
    container_name: spark-batchjob-1
    ports:
      - "4000:4000"
    links:
      - spark-master
      - spark-worker
    volumes:
      - ../build:/usr/build
      - ../src/main/resources:/src/main/resources
      - spark:/spark-master/spark
      - ./spark-scripts:/spark-scripts
    environment:
      - "MAIN_CLASS=spark.spike.Main"
    command: bash -C /spark-scripts/submit_job.sh

volumes:
  spark: {}

